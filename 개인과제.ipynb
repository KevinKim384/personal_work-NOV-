{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e939a36d-efc9-4e82-a872-77f48e87f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용환경을 준비! 참고로 저는 환경변수에 넣었기 때문에(공유가 아니라 제가 이미 돈을 넣어서 제 계정으로 된 API를 쓰고 있습니다!)\n",
    "#os.getenv를 통해 제 환경변수에 있는 API키 가져오기! 그리고 불필요한 경고문 제거를 위해 warnings 추가\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698dcdab-0d85-488d-a76b-73b1b37562ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "client = ChatOpenAI(model = 'gpt-4o', temperature = 0.5, max_tokens = 2000)\n",
    "#PyPDFLoader를 통해 pdf 파일 모델 불러오기!\n",
    "loader = PyPDFLoader('C:\\\\Users\\\\kevinkim\\\\OneDrive\\\\바탕 화면\\\\Sparta_Work\\\\초거대 언어모델 연구 동향.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c2a162-4cfc-4d16-adee-50374676bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#요건 RecursiveCharacterTextSplitter: CharacterTextSplitter와 유사하지만, 더 복잡한 분할 방식을 제공\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #chunk_size: 각 청크의 최대 길이\n",
    "    chunk_size=500,\n",
    "    #chunk_overlap: 청크 간 겹침의 길이\n",
    "    chunk_overlap=10,\n",
    "    #lenth_function: 텍스트 청크의 길이를 계산하는 데 사용하는 함수(기본적으로 len 함수 사용)\n",
    "    length_function=len,\n",
    "    #is_separator_regex: 지정한 구분자가 정규식인지 여부를 지정하는 플래그\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splitted_docs = recursive_text_splitter.split_documents(docs)\n",
    "\n",
    "#이것은 CharacterTextSplitter: 텍스트를 일정한 길이나 규칙에 따라 분할하는 단순한 방법을 제공. \n",
    "#CharacterTextSplitter와 비슷한 방식으로 텍스트를 나누고, \n",
    "#나누어진 조각이 여전히 너무 길면 다시 재귀적으로 분할을 시도\n",
    "#text_splitter = CharacterTextSplitter(\n",
    "    #separator=\"\\n\\n\", -> 텍스트 분할 시 사용할 구분자\n",
    "    #chunk_size=100, -> 설명 완료\n",
    "    #chunk_overlap=10,-> 설명 완료\n",
    "    #length_function=len,-> 설명 완료\n",
    "    #is_separator_regex=False,-> 설명 완료\n",
    "#)\n",
    "#splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8b3607-45f8-452f-b11d-a94614d85dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#너무 길어서 여기는 주석처리로... enumerate을 통해 index와 각 청크를 따로 분리 후, \n",
    "#for i, splitted in enumerate(splitted_docs[:50]):\n",
    "    #print(f'Chunk {i+1}:')\n",
    "    #print(splitted)\n",
    "    #print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f7cf85-f9a8-4a19-bcb0-72e47c3e2647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['93c3ca5a-879c-4fe9-8b2d-786cef50ba2c',\n",
       " '0dd23bc7-ba06-4268-ad7d-fb7d43cf16fe',\n",
       " '7bc6fe9e-0fed-478e-ad20-6bd2a692d241',\n",
       " '45877f0c-884c-4615-8b80-20d63c7bc868',\n",
       " 'dec76e09-c2e3-4258-b1c0-54d47b8dc5fe',\n",
       " '8e84c01e-aba0-44dd-a658-9ec86508cfb5',\n",
       " 'e0d9b153-0047-4131-8887-c15f8da08e64',\n",
       " '1d0d5f79-c82f-4305-9ee4-557e99df8186',\n",
       " 'eff56d3e-fc70-479d-a98d-cec14b22d161',\n",
       " '6b5e0f89-df8f-4582-8f29-a69a8d595fc0',\n",
       " '4280faf4-de4d-4a62-a27d-0d7c43f17d9e',\n",
       " '5dee5749-674a-4b2a-9329-d554bd951d2c',\n",
       " 'd4e1ebbb-d66e-4f93-a04d-52a252c7cf47',\n",
       " '5c2c8757-eae7-45b0-8f6d-50af75e0554b',\n",
       " '52b6c27c-b63d-4af9-9988-1904918b241f',\n",
       " 'b8ce8366-8dcb-45e3-b644-8b31d4926b19',\n",
       " 'c59ac49a-4ac1-49c2-9b8f-1b560efdd10e',\n",
       " 'b7cbac15-1ca2-4dec-9139-1d91a273cac3',\n",
       " '02535324-7c2c-4751-a696-e8da298dbfae',\n",
       " '375607d3-06ee-420b-9774-b9889b4d52e3',\n",
       " '56809f28-dc89-4425-87f3-0c27285a1b72',\n",
       " '0d013851-08d1-4399-8d03-11d52a58a1ea',\n",
       " 'fdcb9b98-b353-42b8-8981-0a50ce808143',\n",
       " '94b94d36-e638-4b7a-825c-6dd37862cb8c',\n",
       " 'bf0b8f7c-40c2-4b10-aa78-951d5a8391b7',\n",
       " '28f79e02-9ff0-4e0f-ac62-8a7d54927f06',\n",
       " '7c6135b6-9b40-45f7-b45d-b7979144b74c',\n",
       " '2924f6a2-891e-4ab9-842e-a8357c07736a',\n",
       " '0ccfd2f6-6858-466e-9889-0ca7dd15188f',\n",
       " 'dbb84f4d-3209-4c0b-ba53-82dcd6295a43',\n",
       " '8b8a1ff1-1f19-406d-94fa-50f82f9d87be',\n",
       " 'b3bb970c-451d-455c-ac18-9501ffefbd8d',\n",
       " 'eca43e82-4fb2-4559-9a4c-e6413cb57d56',\n",
       " '29d8f699-6b0b-4a5a-97fa-c21ba374f967',\n",
       " 'd872f4e5-11d3-4f2e-a93d-9327b62fb214',\n",
       " '1b9ffe9a-a376-4844-b46c-0e2fe8b12d8c',\n",
       " '20165aa0-6e96-469b-aa19-f9d689ff5933',\n",
       " 'f56a2579-912b-43b8-950d-5e436bf314d8',\n",
       " '7f6f6fc2-d223-4a55-b5ad-444e09b632eb',\n",
       " '028cd785-381b-424d-a3fd-36705e614833',\n",
       " 'a4c8cab5-be8c-4ecb-acec-bf6274207aeb',\n",
       " '60a5f9ad-950a-4d0d-ac72-83a51c8cf312',\n",
       " '36a59fb4-c952-4b69-95c8-f25353708f07',\n",
       " '60a6492b-3cef-45c7-9841-4061c221124a',\n",
       " 'df814f13-9aa9-4338-be43-a3680cac5034',\n",
       " '0decdc6d-65a6-40fd-b9d4-6aed4f87edd3',\n",
       " '27edf6fb-7e48-4045-801c-ea309443dd83',\n",
       " '8556b3ed-1fed-4765-a328-63b7de49485a',\n",
       " '1640f506-c33a-4079-87a0-bb69b57dcdd4',\n",
       " '94db3dae-b39b-4a57-bbdc-001763c5d1db',\n",
       " 'b7c4daeb-436e-4333-808e-d8a950907e8b',\n",
       " 'a5e2cb74-a624-4917-9433-85c49807d05a',\n",
       " 'ef13ff6e-eeb3-4d06-9edd-868269ceaefb',\n",
       " '3b0b3dcf-6ef4-45d1-baa1-ca6c1b588152',\n",
       " 'dcdc7986-740d-457e-a8db-b62beeedceea',\n",
       " '677cd4dd-2b01-45fe-b027-c95dffaca851',\n",
       " 'ea4ef0de-e241-40e0-8d2c-4c12297188e4',\n",
       " '456d2740-6c06-4837-828e-e7ce9f2332a1',\n",
       " '036bf91a-859e-4964-a275-07179fc7d69c',\n",
       " '036c720c-09ce-4854-8810-446f6505934a',\n",
       " 'b90ebe6c-0655-4263-9337-4b377273b7f2',\n",
       " '3f5975d0-ea80-47fb-a2e6-97c0cc64001e',\n",
       " '47195450-735a-41a6-ba89-fce49b66b407',\n",
       " 'fbc0f6a1-6a3a-4bf6-a978-4f0feba43926',\n",
       " '541f9333-bb81-426f-aaac-0261e0bdad3e',\n",
       " 'de52f76d-193f-4901-8553-d861e8ff5aef',\n",
       " 'b09d38e3-5fa5-4d37-bfcb-80eb6dad9844',\n",
       " 'b71b7405-22a7-4724-adbf-c1ee866cc9c4',\n",
       " '1fbdbd92-7fbc-4ce8-95a0-d9e569c4badb',\n",
       " '5e411e2a-c294-45d8-b74a-c55c30a8a083',\n",
       " '834c4750-b491-476f-a2c8-5efe39648882',\n",
       " '9271966c-f624-4b7f-8657-7befcdb5c4f4',\n",
       " '44d37c18-af89-4f1c-99bd-41e07abdb89e',\n",
       " '02b049a4-c798-44c4-b515-251041fdcaf4',\n",
       " '89e3c2d4-d571-402c-915f-f6d9c1c36a17',\n",
       " 'bdb238c2-e68e-4544-9b76-32972cebc4e1',\n",
       " 'b5dcf3dc-094d-44be-8995-319cf8814691',\n",
       " '3f023a40-df35-4912-99b3-92a30abaa15d',\n",
       " '9b77078b-69c3-4ac5-b5f2-bcf99d5ebf88',\n",
       " '540070f9-4a5d-4996-b984-baca78c6b478',\n",
       " 'c77430d8-a3fd-4a1b-b456-388eea0a21b4',\n",
       " '800b54c6-64f3-4c48-9724-d05d5ea47793',\n",
       " '7136c2fd-9b99-4e16-bbcb-7da68f8a18bc',\n",
       " '101f8b9e-2236-4593-91c0-c58b3935fecc',\n",
       " '13cd63fd-5e1e-4ba8-b22c-ed962da5ed59',\n",
       " 'e4291d23-0332-402c-83c0-6bc4df6a8d33',\n",
       " '50cf6ef8-c081-4ca0-b0fe-80e40c0e15a2',\n",
       " '500dd2cf-8cfc-4a6e-9a60-d1266cc92abb',\n",
       " '3d1af5bc-c5fb-407a-b1bd-14212ab57a1e',\n",
       " '018d182f-6d87-4454-821b-0de9704b0f20',\n",
       " '735aaae6-ea9e-41ee-bd7c-01e11241cc9f',\n",
       " '2442b934-ebc4-4225-a320-2b11e3f1e2ae',\n",
       " '81319b75-4492-436d-9242-8f208dce6b20',\n",
       " '7c7c037a-fb7c-4322-a72b-0be19e606545',\n",
       " '8a4db3d6-92a4-45f8-8179-6aac910e7900',\n",
       " '1fb4f9a3-56ec-415b-aee6-047effcd74c6',\n",
       " '4690c64f-9cd8-4eca-81ca-f45faf7b97f0',\n",
       " 'ca617935-0454-405c-ba20-3932ab3fd897',\n",
       " '0ce2ab0b-d097-4f60-acae-d6b11aa9add1',\n",
       " 'a41e533e-ad41-4d68-b417-861975dd912e',\n",
       " 'c7bce4f9-e287-4bdd-a2e7-a83bba14f059',\n",
       " 'f63583fe-6cd0-491c-8d84-c342487fddd2',\n",
       " 'fcf7b30e-203c-4aff-aeb6-ffe205c7d594',\n",
       " '0f29790a-99c1-48dc-8c72-c39bfbe4cd08',\n",
       " '0b26707e-8574-48fc-8365-29a82811b951',\n",
       " 'fe1d25f1-0307-4a63-ab54-60b4d9fbd576',\n",
       " '89807217-5b16-4dd0-a0b9-4c37f779c4f7',\n",
       " '72cc093d-8fb2-4e8b-a69d-aab41bbbce56',\n",
       " '56cc96a3-0679-40d2-81b6-252c679cf963',\n",
       " 'f4988279-3f1e-4c00-9a82-5651cfe535ef',\n",
       " '4aad2bc7-4398-4962-992e-d7c1e47f28a8',\n",
       " '9c043e6c-02fc-4d8b-95fe-032bdfb17856',\n",
       " '90253686-6b1a-4cde-b127-cc7ee08f76c3',\n",
       " 'f1ef4677-eedc-4ce0-b637-2f9e94ff67b8',\n",
       " '735a6423-56c7-4854-b8ec-fd32b277b67d',\n",
       " '1d292b40-259e-4cf5-80a3-34652171129f',\n",
       " '2f8c24e3-bf11-4bc2-933f-19656631d789',\n",
       " '385a7640-b4a2-4823-81b2-482aef726c5b',\n",
       " 'af122f08-97dd-43e8-a8d6-a544fb652a14',\n",
       " 'dcaec127-8e3b-4193-b3b2-c82e2fda857d',\n",
       " 'ee841559-7aea-440a-bb2c-688be70a348d']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from uuid import uuid4\n",
    "import faiss\n",
    "from langchain.vectorstores import Chroma\n",
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-ada-002')\n",
    "\n",
    "#from_documents를 통해 쉽게 vector_store 생성\n",
    "vector_store = Chroma.from_documents(documents = splitted_docs, embedding = embeddings)\n",
    "#심심하니 uuid4를 사용해 이미 chunk로 변한 값들을 고유한 코드로 된 값들로 다시 변경(이러면 PC가 사용하기 더 쉬워진다나 뭐라나...)\n",
    "uuids = [str(uuid4()) for _ in range(len(splitted_docs))]\n",
    "vector_store.add_documents(documents = splitted_docs, ids = uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8446ae64-119b-4a97-8e26-32a2841a46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "#여기서는 평병하게 retriever를 구성하였습니다.\n",
    "#Chroma에서 이 부분 llm = client를 통해 쉽게 LLM에 연결이 가능합니다.\n",
    "retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = {'k':5})\n",
    "#여기서는 RetrieveralQA를 써서 질문과 답변을 받을 수 있게 만드는 함수\n",
    "qa_retriever = RetrievalQA.from_chain_type(\n",
    "    llm = client,\n",
    "    chain_type = 'refine',\n",
    "    retriever = retriever,\n",
    "    return_source_documents = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18967d49-becd-47ed-8caa-b91fd12fec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 해주세요: 초거대 언어모델의 성능향상을 위한 연구에 대해 알려줘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 초거대 언어모델의 성능 향상을 위한 연구는 다양한 측면에서 진행되고 있으며, 이는 언어모델의 발전 과정을 이해하는 데 중요한 배경을 제공합니다. ChatGPT와 같은 초거대 언어모델의 등장은 기존에 병렬적으로 연구되던 다양한 자연언어처리(NLP) 하위 분야를 하나의 모델로 처리할 수 있게 하면서, 태스크 수렴 현상을 가져왔습니다. 즉, 하나의 LLM을 통해 번역, 요약, 질의응답, 형태소 분석 등의 작업을 모두 처리할 수 있게 되었습니다.\n",
      "\n",
      "이러한 발전은 주로 프롬프트를 어떻게 입력하느냐에 따라 모델의 다양한 능력이 창발되는 점에 기반하고 있습니다. 사용자는 프롬프트를 조정함으로써 모델이 특정 태스크에 맞는 출력을 생성하도록 할 수 있습니다. 이를 통해 LLM은 사용자의 목적에 맞는 다양한 출력을 생성할 수 있는 새로운 패러다임을 제공하고 있습니다.\n",
      "\n",
      "또한, 데이터의 질과 양을 향상시키는 연구는 모델의 성능을 높이는 데 필수적입니다. 방대한 양의 데이터를 효과적으로 활용하기 위한 방법론이 개발되고 있으며, 이는 모델의 학습과 일반화 능력을 향상시키는 데 기여합니다. 최신 초거대 언어모델 연구 동향을 심층적으로 살펴보면, 새로운 평가 방식과 기준이 등장하고 있으며, 이는 모델의 다양한 언어적 과제 수행 능력을 측정하고 실제 응용 사례에서의 성능을 개선하는 데 도움을 줍니다.\n",
      "\n",
      "마지막으로, 초거대 언어모델의 윤리적 사용과 관련된 연구도 중요하게 다루어지고 있습니다. 이는 모델이 편향되지 않도록 하고, 사용자에게 해를 끼치지 않도록 보장하는 방법을 찾는 것을 포함합니다. 이러한 연구들은 초거대 언어모델이 더 안전하고 책임감 있게 사용될 수 있도록 하는 데 기여하며, 연구자와 관련 전문가들에게 유용한 통찰과 지침을 제공합니다.\n",
      "\n",
      "이와 같은 연구들은 초거대 언어모델의 전반적인 동향과 중요한 주제들에 대한 체계적인 이해를 제공하며, 자연언어처리의 발전과 함께 지속적으로 발전하고 있습니다. 이를 통해 연구자들은 더욱 향상된 모델을 개발하고, 다양한 언어적 과제를 해결하는 데 필요한 도구와 방법론을 마련할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "#만약 질문을 계속 받을시에 작동할 while True: 하지만 비교를 위해 못 쓴다는...\n",
    "#query로 답을 받아, 그것을 토대로 값을 출\n",
    "#while True:\n",
    "query = input('질문을 해주세요:')\n",
    "#if query == 'exit':\n",
    "    #print('좋은 대화였습니다!')\n",
    "    #break\n",
    "result = qa_retriever(query)\n",
    "answer_a = result['result']\n",
    "source_documents = result['source_documents']\n",
    "\n",
    "    # 결과 출력\n",
    "print(\"Answer:\", answer_a)\n",
    "#하지만 QA특성상 추출할 값도 가져와야 하는데, 값을 가져오고 띄우지는 못하게 하는 코드\n",
    "logging.info(\"Source Documents: %s\", source_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0627f3f1-e7d7-4485-a158-1a19220b9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초거대 언어 모델(Giant Language Model)은 대규모 자연어 처리(NLP) 문제를 해결하기 위해 개발된 인공지능 모델입니다. 이러한 모델들은 방대한 양의 텍스트 데이터를 학습하여 인간과 유사한 방식으로 언어를 이해하고 생성할 수 있도록 설계되었습니다. 여기에는 문서 요약, 번역, 질문 응답, 대화 생성 등의 다양한 작업이 포함됩니다. 최근 몇 년간 주목할 만한 초거대 언어 모델에는 다음과 같은 것들이 있습니다.\n",
      "\n",
      "1. **GPT 시리즈**: 오픈AI가 개발한 모델로서, 다양한 언어 처리 작업에서 혁신적인 성능을 보여주었습니다. 특히 GPT-3와 GPT-4는 각각 155억 개와 수천억 개의 매개변수를 가진 모델로, 자연어 생성에서 뛰어난 능력을 보입니다.\n",
      "\n",
      "2. **BERT 및 확장 버전**: 구글이 개발한 BERT는 양방향 트랜스포머 기반의 언어 모델입니다. 이후 여러 확장 모델이 개발되어, BERT의 성능을 향상시키거나 특정 작업에 맞추어 특화된 형태로 발전하였습니다.\n",
      "\n",
      "3. **T5 (Text-to-Text Transfer Transformer)**: 이 모델은 대부분의 NLP 작업을 텍스트 입력을 텍스트 출력으로 변환하는 일관된 접근 방식으로 통합하여 수행합니다. 이 모델은 다양한 언어 이해 및 생성 작업에서 뛰어난 성과를 보여줍니다.\n",
      "\n",
      "4. **LaMDA (Language Model for Dialogue Applications)**: 구글이 개발한 대화용 언어 모델로, 인간과 같은 대화를 생성하는 데 특화되어 있습니다.\n",
      "\n",
      "5. **Claude**: Anthropic에서 개발한 모델로, 인간 중심의 AI 안전성을 중점으로 개발되었습니다. 대규모 언어 모델을 개발하는 데 있어 윤리와 안전성을 강조합니다.\n",
      "\n",
      "6. **LLaMA (Large Language Model Meta AI)**: Meta AI에서 개발한 모델로, 연구 공동체가 쉽게 접근할 수 있도록 공개되었습니다. 다양한 크기의 버전이 존재하며, 연구 및 사용자 지정 작업에 사용됩니다.\n",
      "\n",
      "최근의 연구 동향은 이러한 대규모 언어 모델의 사회적 영향, 윤리적 사용, 에너지 소비에 관한 논의와 개선을 포함합니다. 모델의 투명성과 공정성을 보장하고, 사전 학습 데이터의 편향을 줄이려는 다양한 시도가 진행되고 있습니다. 또한, 모델의 효율성을 높이기 위해 경량화 및 조정 기법이 활발히 연구되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "#일반 LLM 모델인 GPT를 이용해 같은 질문을 받아 출력 비교\n",
    "from openai import OpenAI\n",
    "client2 = OpenAI()\n",
    "completion = client2.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = [\n",
    "        {'role' : 'system', 'content' : '넌 초거대 언어모델 연구 동향에 대해 설명해주는 AI야. 여러 관련 모델들과 지식들을 설명해줘.'},\n",
    "        {'role' : 'user', 'content' : 'query'}\n",
    "    ]\n",
    ")\n",
    "answer_b = completion.choices[0].message.content\n",
    "print(answer_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de8637d8-2ead-477b-a481-b68bed15d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Assistant A and Assistant B provide informative answers about the research efforts to enhance the performance of large language models. However, they focus on different aspects of the topic.\n",
      "\n",
      "Assistant A offers a detailed overview of the research aimed at improving the performance of large language models, discussing the integration of different NLP tasks into a single model, the role of prompt engineering, improvements in data quality and quantity, new evaluation methods, and ethical considerations. The response covers various dimensions of research, demonstrating an understanding of the complexity and scope of advancements in large language models. The discussion on prompt engineering, data methodologies, and ethical research adds depth to the response.\n",
      "\n",
      "Assistant B, on the other hand, provides a brief overview of several prominent language models, including GPT, BERT, T5, LaMDA, Claude, and LLaMA. The response also touches on recent research trends, including social impact, ethical usage, energy consumption, transparency, and efficiency improvements. While Assistant B lists several models and describes their contributions, it does not delve into specific research strategies or methodologies as extensively as Assistant A.\n",
      "\n",
      "In summary, Assistant A's response is more focused on the research processes and methodologies that contribute to improving large language models, offering a comprehensive picture of the ongoing efforts in the field. Assistant B lists specific models and general research trends but lacks the detailed exploration of research strategies found in Assistant A's response.\n",
      "\n",
      "Therefore, Assistant A's response is more aligned with the research-focused nature of the user's query and provides more depth and insight into how performance improvements are achieved.\n",
      "\n",
      "[[A]]\n"
     ]
    }
   ],
   "source": [
    "#A가 더 좋다는 답변을 받았다! 즉, 내가 만든 값이 더 정확하고 좋다는 뜻!\n",
    "prompt = f\"\"\"[System]\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the user's instructions and answers the user's question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\n",
    "\n",
    "[User Question]\n",
    "{query}\n",
    "\n",
    "[The Start of Assistant A's Answer]\n",
    "{answer_a}\n",
    "[The End of Assistant A's Answer]\n",
    "\n",
    "[The Start of Assistant B's Answer]\n",
    "{answer_b}\n",
    "[The End of Assistant B's Answer]\"\"\"\n",
    "\n",
    "completion = client2.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = [\n",
    "        {'role' : 'user', 'content' : prompt}\n",
    "    ]\n",
    ")\n",
    "final = completion.choices[0].message.content\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b054a5c-419a-44e6-a0c8-bd04d07f7ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG 모델은 먼저 입력된 질문과 관련된 정보를 검색하고, 그 정보를 바탕으로 답변을 생성합니다. \\n이렇게 하면 모델이 정확하고 최신의 정보를 기반으로 답변을 생성할 수 있습니다.\\n특히 Chroma와 같은 벡터 데이터베이스를 사용하여 RAG 모델을 구현할 때, 모델의 성능을 크게 향상시킬 수 있습니다.\\n특히 Chroma는 벡터 데이터베이스로, 문서나 텍스트 데이터를 벡터화하여 빠르고 효율적인 검색을 지원하는데, RAG 모델에서 Chroma와 같은 검색 시스템을 사용하면, \\n모델이 실시간으로 관련 정보를 빠르게 검색하고, 그 정보를 바탕으로 더 적합한 답변을 생성할 수 있습니다.\\n한 마디로 LLM 같은 대규모 랭귀지 모델에서 RAG를 통해 최신 정보를 토대로 정확한 답을 생성해낼 수 있기 때문입니다.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"RAG 모델은 먼저 입력된 질문과 관련된 정보를 검색하고, 그 정보를 바탕으로 답변을 생성합니다. \n",
    "이렇게 하면 모델이 정확하고 최신의 정보를 기반으로 답변을 생성할 수 있습니다.\n",
    "특히 Chroma와 같은 벡터 데이터베이스를 사용하여 RAG 모델을 구현할 때, 모델의 성능을 크게 향상시킬 수 있습니다.\n",
    "특히 Chroma는 벡터 데이터베이스로, 문서나 텍스트 데이터를 벡터화하여 빠르고 효율적인 검색을 지원하는데, RAG 모델에서 Chroma와 같은 검색 시스템을 사용하면, \n",
    "모델이 실시간으로 관련 정보를 빠르게 검색하고, 그 정보를 바탕으로 더 적합한 답변을 생성할 수 있습니다.\n",
    "한 마디로 LLM 같은 대규모 랭귀지 모델에서 RAG를 통해 최신 정보를 토대로 정확한 답을 생성해낼 수 있기 때문입니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11fb1c19-413c-4410-b6df-30d44b9f3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.14432989690721648, 'p': 0.1590909090909091, 'f': 0.15135134636318495}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.018018018018018018, 'f': 0.01724137431963581}, 'rouge-l': {'r': 0.13917525773195877, 'p': 0.1534090909090909, 'f': 0.14594594095777957}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "score = rouge.get_scores(answer_a, answer_b)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43d4a1-b232-4477-8545-c764184af9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
